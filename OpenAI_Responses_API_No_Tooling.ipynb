{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKWYRX6kWeuv"
   },
   "source": [
    "# OpenAI Responses API\n",
    "\n",
    "Welcome to AI Makerspace! This notebook demonstrates the powerful new **OpenAI Responses API**, which provides a streamlined interface for working with OpenAI's language models without the complexity of tool calling.\n",
    "\n",
    "The Responses API offers several key advantages:\n",
    "- **Simplified interface** - Direct text generation without tool schemas\n",
    "- **Built-in reasoning controls** - Adjust effort levels for different use cases  \n",
    "- **Structured output parsing** - Native Pydantic model support\n",
    "- **Multimodal capabilities** - Text and image inputs in a single request\n",
    "- **Streaming support** - Real-time response generation\n",
    "- **Developer instructions** - System-level guidance separate from user content\n",
    "\n",
    "This notebook walks through the core features with practical examples, showing how you can integrate this API into your AI applications for cleaner, more maintainable code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWTBWGsdWeuw"
   },
   "source": [
    "### Setup and Authentication\n",
    "\n",
    "First, we need to set up our OpenAI API credentials. We'll use `getpass` to securely input the API key without exposing it in the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYqwOef1Weuw"
   },
   "source": [
    "Now we'll initialize the OpenAI client using our API key. This client will be used for all subsequent API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zCusHswP-lX",
    "outputId": "dbd8b121-72cc-406f-8d25-bac2dcd7914e"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YAXziP4OQqVN"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcI8DQiNWeux"
   },
   "source": [
    "### Basic Response Generation\n",
    "\n",
    "Let's start with a simple example using the new `responses.create()` method. This is the core interface for the Responses API - notice how clean and straightforward it is compared to the traditional chat completions API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAEIt5nVQEFk",
    "outputId": "e2e03f31-3cf8-44d3-f445-cdc3f64c8be5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI engineering is the discipline of designing, building, deploying, and operating AI-enabled systems using rigorous software and systems engineering practices so they work reliably, safely, and at scale in real-world contexts.\n",
      "\n",
      "It blends machine learning, data engineering, and software engineering to manage the full lifecycle of AI, including:\n",
      "- Data management: collection, labeling, quality, lineage, and governance\n",
      "- Model development: training, evaluation, experiment tracking, and reproducibility\n",
      "- Infrastructure: training/serving platforms, performance, cost, and scalability\n",
      "- MLOps: pipelines, CI/CD for models, monitoring, alerts, rollback, and drift handling\n",
      "- Safety and trust: robustness, security, privacy, fairness, interpretability, and compliance\n",
      "- Human-in-the-loop and UX: integrating AI with user workflows and oversight\n",
      "- Product integration: APIs, services, edge/cloud deployment, and architecture\n",
      "- Lifecycle governance: versioning, documentation, audits, and incident response\n",
      "\n",
      "In short, AI engineering turns AI models into dependable, maintainable, and governed products and services.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Define what 'AI Engineering' is.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyqyQaEeWeux"
   },
   "source": [
    "### Reasoning Control and Instructions\n",
    "\n",
    "One of the powerful features of the Responses API is the ability to control the reasoning effort and provide developer instructions. Here we're using:\n",
    "\n",
    "- `reasoning={\"effort\": \"low\"}` - Controls how much computational effort the model puts into reasoning\n",
    "- `instructions` - Developer-level instructions that guide the model's behavior (separate from user content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdBe3I03Qbw3",
    "outputId": "55d77342-87e1-42fd-9c23-fb3c0d90a513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short answer: in NumPy, the most efficient “loop” is no Python loop at all. You push the work into vectorized, compiled NumPy operations and BLAS calls. Here’s how to do that in practice.\n",
      "\n",
      "What to do instead of loops\n",
      "- Use vectorized ufuncs for elementwise math.\n",
      "  - Example: y = a*x + b becomes y = a * x + b (x is an array).\n",
      "- Use broadcasting to align shapes without copying.\n",
      "  - Example: add a column vector to every row: A + b[None, :]\n",
      "- Use reductions and aggregations provided by NumPy.\n",
      "  - sum, mean, min/max, argmin/argmax, cumsum, nansum, etc.\n",
      "- Prefer linear algebra operations that call BLAS/LAPACK.\n",
      "  - A @ B, np.dot, np.linalg.solve, np.linalg.norm, np.einsum/np.tensordot.\n",
      "- Use boolean masks instead of looping over conditions.\n",
      "  - y = np.where(x > 0, x, 0); A[A < 0] = 0\n",
      "- Use advanced indexing rather than per-element assignments.\n",
      "  - A[idx_rows[:, None], idx_cols] = values\n",
      "- For sliding windows, avoid loops with stride tricks.\n",
      "  - windows = np.lib.stride_tricks.sliding_window_view(x, window_shape)\n",
      "\n",
      "Patterns and examples\n",
      "1) Elementwise transform\n",
      "- Python loop (slow)\n",
      "  for i in range(n): y[i] = a*x[i] + b\n",
      "- NumPy\n",
      "  y = a * x + b\n",
      "\n",
      "2) Row-wise operations\n",
      "- Compute row-wise z-scores without looping:\n",
      "  mu = A.mean(axis=1, keepdims=True)\n",
      "  sd = A.std(axis=1, keepdims=True)\n",
      "  z = (A - mu) / sd\n",
      "\n",
      "3) Pairwise distances (avoid double loop)\n",
      "- For 2D arrays X (n,d) and Y (m,d):\n",
      "  # squared Euclidean\n",
      "  XX = (X**2).sum(axis=1, keepdims=True)\n",
      "  YY = (Y**2).sum(axis=1)[None, :]\n",
      "  D2 = XX + YY - 2 * (X @ Y.T)\n",
      "\n",
      "4) Weighted sum across last axis with einsum\n",
      "- Instead of loop over k:\n",
      "  y = np.einsum('...k,k->...', A, w)\n",
      "\n",
      "5) Grouped operations\n",
      "- If groups are small set of labels, use bincount/accumulation:\n",
      "  sums = np.bincount(labels, weights=values, minlength=K)\n",
      "  counts = np.bincount(labels, minlength=K)\n",
      "  means = sums / counts\n",
      "\n",
      "6) Sliding window rolling mean\n",
      "- No loop:\n",
      "  w = 5\n",
      "  W = np.lib.stride_tricks.sliding_window_view(x, w)\n",
      "  roll_mean = W.mean(axis=-1)\n",
      "\n",
      "Practical guidance\n",
      "- Preallocate outputs. Avoid growing arrays in a loop (np.append inside a loop is a red flag). If you must loop, do:\n",
      "  out = np.empty(n); for i in range(n): out[i] = ...\n",
      "- Keep arrays numeric and homogeneous. Avoid dtype=object.\n",
      "- Mind memory layout. Vectorized code can still be memory-bound.\n",
      "  - Prefer contiguous arrays when possible: x = np.ascontiguousarray(x)\n",
      "  - Work along contiguous axes for better cache behavior.\n",
      "- Use broadcasting consciously. Check shapes with .shape and add axes via None/np.newaxis rather than np.tile (which copies).\n",
      "- Prefer built-ins over Python helpers:\n",
      "  - np.vectorize and np.apply_along_axis do not speed things up; they just wrap loops. Only use for convenience on small data.\n",
      "  - np.nditer doesn’t inherently speed code; it’s still Python-level looping.\n",
      "- Chunk if data doesn’t fit in RAM. Process in blocks that fit cache/LLC when doing big matmuls or reductions.\n",
      "\n",
      "When a loop is unavoidable\n",
      "- If your operation truly needs Python-level logic per element:\n",
      "  - Try numba @njit on a function operating on NumPy arrays.\n",
      "  - Or write a small Cython/pybind11 kernel.\n",
      "  - Within NumPy only, prefer iterating with np.ndindex over nested Python loops, writing into a preallocated array.\n",
      "\n",
      "Performance checklist\n",
      "- Can I express this with ufuncs, broadcasting, reductions, or einsum?\n",
      "- Am I avoiding Python loops, list appends, and per-element Python calls?\n",
      "- Are my arrays the right dtype and contiguous?\n",
      "- Am I using BLAS-backed operations where applicable?\n",
      "- Have I profiled? Use %timeit and line_profiler to confirm.\n",
      "\n",
      "If you share a concrete loop you’re trying to speed up, I’ll rewrite it in vectorized form and estimate the speedup.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning={\"effort\": \"low\"},\n",
    "    instructions=\"Talk like a manager.\",\n",
    "    input=\"How to write an efficient loop with NumPy?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ycx8tykjWeux"
   },
   "source": [
    "### Message-Based Input Format\n",
    "\n",
    "The Responses API also supports the familiar message format with roles. Here we demonstrate:\n",
    "\n",
    "- Using `input` as a list of message objects instead of a simple string\n",
    "- `developer` role - A new role type for system-level instructions\n",
    "- `user` role - Standard user input\n",
    "\n",
    "This approach gives you more granular control over the conversation structure while maintaining the simplified API interface.\n",
    "\n",
    "> NOTE: This should be *roughly* equivalent to the above cell - since we're using the same \"instructions\" in our `developer` role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g1c3IhkyQnG7",
    "outputId": "ca083b8c-7eca-4f29-b518-52ea15cc815d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, seeker of speed, heed these arcane runes: with NumPy, the mightiest loop is oft no loop at all. Invoke the powers of vectorization and broadcasting, and your spells shall fly.\n",
      "\n",
      "Principles of efficiency\n",
      "- Prefer whole-array operations (vectorization) over Python for-loops.\n",
      "- Use broadcasting to combine arrays without explicit iteration.\n",
      "- Wield ufuncs (universal functions) and reductions (sum, mean, min, max) along axes.\n",
      "- Preallocate arrays; shun repeated append/concatenate within loops.\n",
      "- Favor in-place updates (out=..., arr += ...) to spare memory.\n",
      "- Keep dtypes numeric (no object), and arrays contiguous when possible.\n",
      "- If a Python loop is truly needed, consider Numba to JIT-compile it.\n",
      "\n",
      "Common patterns and their incantations\n",
      "1) Elementwise transformations\n",
      "- Slow loop:\n",
      "  for i in range(n): y[i] = a*x[i] + b\n",
      "- Fast vectorization:\n",
      "  y = a*x + b\n",
      "\n",
      "2) Conditional logic\n",
      "- if/else per element:\n",
      "  y = np.where(x > 0, np.log1p(x), 0.0)\n",
      "- Boolean masks:\n",
      "  y = np.empty_like(x)\n",
      "  mask = x > 0\n",
      "  y[mask] = np.log1p(x[mask])\n",
      "  y[~mask] = 0.0\n",
      "\n",
      "3) Reductions along axes\n",
      "- Summon aggregates without looping:\n",
      "  col_sums = x.sum(axis=0)\n",
      "  row_max = x.max(axis=1)\n",
      "\n",
      "4) Broadcasting to avoid nested loops\n",
      "- Pairwise distances (N×D vs M×D) without double loops:\n",
      "  # squared Euclidean\n",
      "  # X: (N,D), Y: (M,D)\n",
      "  d2 = ((X[:, None, :] - Y[None, :, :])**2).sum(axis=2)\n",
      "\n",
      "5) Matrix magic\n",
      "- Use BLAS-backed ops:\n",
      "  C = A @ B          # matrix multiply\n",
      "  x = A @ v          # matrix-vector\n",
      "\n",
      "6) Scatter/gather\n",
      "- Binning or grouped adds:\n",
      "  out = np.zeros(k)\n",
      "  np.add.at(out, idx, values)   # idx are bin indices\n",
      "- Searching sorted arrays:\n",
      "  pos = np.searchsorted(edges, x, side=\"right\") - 1\n",
      "\n",
      "7) Running/rolling computations\n",
      "- Cumulative:\n",
      "  cs = np.cumsum(x)\n",
      "- Rolling windows:\n",
      "  from numpy.lib.stride_tricks import sliding_window_view\n",
      "  w = sliding_window_view(x, window_shape=5)\n",
      "  roll_mean = w.mean(axis=-1)\n",
      "\n",
      "8) Avoid Python loops when accumulating\n",
      "- Bad:\n",
      "  out = []\n",
      "  for chunk in chunks: out.append(f(chunk))\n",
      "  out = np.concatenate(out)   # costly growth pattern\n",
      "- Better: compute shapes, preallocate, or stack once:\n",
      "  out = np.concatenate([f(c) for c in chunks], axis=0)\n",
      "\n",
      "9) In-place and memory-wise spellcraft\n",
      "- In-place ufuncs:\n",
      "  np.multiply(x, 2, out=x)\n",
      "  np.add(x, y, out=x)\n",
      "- Avoid unnecessary copies:\n",
      "  y = x.T.copy()   # only if you must; x.T is a view otherwise\n",
      "- Ensure dtype and order suit your rites:\n",
      "  x = np.ascontiguousarray(x, dtype=np.float64)\n",
      "\n",
      "10) When a loop is inevitable\n",
      "- NumPy’s np.vectorize is but syntactic sugar; it does not speed things up.\n",
      "- Use Numba for tight numeric loops:\n",
      "  from numba import njit\n",
      "  @njit\n",
      "  def loop_spell(a, b):\n",
      "      out = np.empty_like(a)\n",
      "      for i in range(a.size):\n",
      "          out[i] = a[i]*a[i] + b\n",
      "      return out\n",
      "\n",
      "Tiny exemplars\n",
      "\n",
      "- Threshold and normalize rows:\n",
      "  # X: (N,D). Set negatives to 0, then L2-normalize per row.\n",
      "  X = np.maximum(X, 0)\n",
      "  norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
      "  X = X / np.clip(norms, 1e-12, None)\n",
      "\n",
      "- Argmax per group (group ids in g, values v):\n",
      "  # groups numbered 0..G-1\n",
      "  G = g.max() + 1\n",
      "  best_idx = np.full(G, -1, dtype=np.int64)\n",
      "  best_val = np.full(G, -np.inf)\n",
      "  # single pass with where:\n",
      "  better = v > best_val[g]\n",
      "  np.copyto(best_val[g], v, where=better)\n",
      "  np.copyto(best_idx[g], np.arange(v.size), where=better)\n",
      "\n",
      "Performance lore\n",
      "- Measure with %timeit (in IPython) on realistic data sizes.\n",
      "- Large temporary arrays may dominate runtime; fuse operations when possible.\n",
      "- Use float32 for large arrays if precision allows; halves memory bandwidth.\n",
      "\n",
      "Speak these incantations, and your loops shall dissolve into swift array sorcery. If you share a specific loop, I shall transmute it into NumPy’s tongue.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning={\"effort\": \"low\"},\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Talk like a wizard.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How to write an efficient loop with NumPy?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_g8cEf2Weux"
   },
   "source": [
    "### Structured Output with Pydantic\n",
    "\n",
    "One of the most exciting features is native structured output parsing using Pydantic models. Instead of trying to parse JSON from text responses, the API can directly return structured data.\n",
    "\n",
    "Key features:\n",
    "- `responses.parse()` method for structured output\n",
    "- `text_format` parameter accepts Pydantic models\n",
    "- Automatic validation and type checking\n",
    "- Clean, typed responses that integrate seamlessly with Python applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rN_YzMK8Q7wd",
    "outputId": "3afea092-007d-4a7d-9199-8d6dbf0f8228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='AI Engineering Bootcamp kickoff' date='September 9 at 7:00 PM' participants=['Alice', 'Bob']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"Extract the event information.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to the AI Engineering Bootcamp kickoff on September 9th at 7PM. Registration closes Tuesday (9/9) at Noon EDT.\",\n",
    "        },\n",
    "    ],\n",
    "    text_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "print(response.output_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUpvG3UbWeux"
   },
   "source": [
    "### Multimodal Input - Text and Images\n",
    "\n",
    "The Responses API provides excellent support for multimodal inputs, allowing you to combine text and images in a single request. This example shows:\n",
    "\n",
    "- `input_text` and `input_image` content types\n",
    "- Direct image URL support\n",
    "- Unified processing of text and visual information\n",
    "\n",
    "This makes it easy to build applications that need to analyze images, documents, or other visual content alongside text instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJqn5d6eRGpx",
    "outputId": "9471a0a6-2843-44b7-ed24-39ef1574273b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Left person: Light-skinned man, smiling with teeth showing. He wears a black hoodie and a black-and-white patterned baseball cap facing forward. Short facial hair/stubble. He’s outlined with a pink glow.\n",
      "- Right person: Light-skinned man with a short goatee and mustache, hair pulled back. He’s smiling softly with arms crossed. He wears a blue-and-white tie-dye style polo shirt and a dark smartwatch on his left wrist. He’s outlined with a blue glow.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Describe the visual appearance of the people in this image.\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": \"https://d2426xcxuh3ht5.cloudfront.net/rnDUeTj9Sqysz9tCeGdB_AIE_Cohort_7_Banner.jpg\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbCclIQ7Weux"
   },
   "source": [
    "### Streaming Responses with Structured Output\n",
    "\n",
    "For real-time applications, the Responses API supports streaming even with structured output. This example demonstrates:\n",
    "\n",
    "- `responses.stream()` context manager for streaming\n",
    "- Event-based streaming with different event types:\n",
    "  - `response.output_text.delta` - Incremental text updates\n",
    "  - `response.refusal.delta` - Safety refusal messages  \n",
    "  - `response.error` - Error handling\n",
    "  - `response.completed` - Stream completion\n",
    "- `get_final_response()` to retrieve the complete parsed result\n",
    "- Structured output streaming with Pydantic models\n",
    "\n",
    "This is perfect for building responsive UIs that show progress while maintaining type safety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wD91RQ79Rupf",
    "outputId": "f5e9d98f-dcb5-4d43-9bcb-17a12d1ec767"
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Your organization must be verified to stream this model. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': 'stream', 'code': 'unsupported_value'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     colors: List[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m      6\u001b[39m     animals: List[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-5-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExtract entities from the input text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe quick brown fox jumps over the lazy dog with piercing blue eyes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEntitiesModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse.refusal.delta\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AIE8/AIE8/.venv/lib/python3.11/site-packages/openai/lib/streaming/responses/_responses.py:111\u001b[39m, in \u001b[36mResponseStreamManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ResponseStream[TextFormatT]:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     raw_stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__api_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.__stream = ResponseStream(\n\u001b[32m    114\u001b[39m         raw_stream=raw_stream,\n\u001b[32m    115\u001b[39m         text_format=\u001b[38;5;28mself\u001b[39m.__text_format,\n\u001b[32m    116\u001b[39m         input_tools=\u001b[38;5;28mself\u001b[39m.__input_tools,\n\u001b[32m    117\u001b[39m         starting_after=\u001b[38;5;28mself\u001b[39m.__starting_after,\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AIE8/AIE8/.venv/lib/python3.11/site-packages/openai/resources/responses/responses.py:828\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    792\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    793\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    826\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    827\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m828\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AIE8/AIE8/.venv/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/AIE8/AIE8/.venv/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Your organization must be verified to stream this model. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': 'stream', 'code': 'unsupported_value'}}"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "class EntitiesModel(BaseModel):\n",
    "    attributes: List[str]\n",
    "    colors: List[str]\n",
    "    animals: List[str]\n",
    "\n",
    "with client.responses.stream(\n",
    "    model=\"gpt-5-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract entities from the input text\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"The quick brown fox jumps over the lazy dog with piercing blue eyes\",\n",
    "        },\n",
    "    ],\n",
    "    text_format=EntitiesModel,\n",
    ") as stream:\n",
    "    for event in stream:\n",
    "        if event.type == \"response.refusal.delta\":\n",
    "            print(event.delta, end=\"\")\n",
    "        elif event.type == \"response.output_text.delta\":\n",
    "            print(event.delta, end=\"\")\n",
    "        elif event.type == \"response.error\":\n",
    "            print(event.error, end=\"\")\n",
    "        elif event.type == \"response.completed\":\n",
    "            print(\"Completed\")\n",
    "            # print(event.response.output)\n",
    "\n",
    "    final_response = stream.get_final_response()\n",
    "    print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rf-wLzEWeux"
   },
   "source": [
    "## Summary\n",
    "\n",
    "The OpenAI Responses API represents a significant step forward in making AI integration simpler and more powerful. Key takeaways:\n",
    "\n",
    "✅ **Cleaner code** - Less boilerplate, more focus on your application logic  \n",
    "✅ **Built-in structure** - Native Pydantic support eliminates JSON parsing headaches  \n",
    "✅ **Flexible control** - Fine-tune reasoning effort and output style  \n",
    "✅ **Multimodal ready** - Text and image inputs work seamlessly together  \n",
    "✅ **Production ready** - Streaming support for responsive applications  \n",
    "\n",
    "---\n",
    "\n",
    "### Advanced Configuration Options\n",
    "\n",
    "The final example showcases some advanced configuration options:\n",
    "\n",
    "- `reasoning={\"effort\": \"minimal\"}` - Even lower computational effort for simple tasks\n",
    "- `text={\"verbosity\": \"low\"}` - Control output verbosity for concise responses\n",
    "- Fine-tuning the balance between speed, cost, and output quality\n",
    "\n",
    "This final configuration will give you the closest possible performance to GPT-4o, just in case you miss the old model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nvl7dNQkSLgG",
    "outputId": "2f15dc71-5ffb-4aee-adf9-e7b2d97840d7"
   },
   "outputs": [],
   "source": [
    "result = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    input=\"Write a haiku about code.\",\n",
    "    reasoning={ \"effort\": \"minimal\" },\n",
    "    text={ \"verbosity\": \"low\" },\n",
    ")\n",
    "\n",
    "print(result.output_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
