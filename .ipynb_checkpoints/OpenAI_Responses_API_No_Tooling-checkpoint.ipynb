{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKWYRX6kWeuv"
      },
      "source": [
        "# OpenAI Responses API\n",
        "\n",
        "Welcome to AI Makerspace! This notebook demonstrates the powerful new **OpenAI Responses API**, which provides a streamlined interface for working with OpenAI's language models without the complexity of tool calling.\n",
        "\n",
        "The Responses API offers several key advantages:\n",
        "- **Simplified interface** - Direct text generation without tool schemas\n",
        "- **Built-in reasoning controls** - Adjust effort levels for different use cases  \n",
        "- **Structured output parsing** - Native Pydantic model support\n",
        "- **Multimodal capabilities** - Text and image inputs in a single request\n",
        "- **Streaming support** - Real-time response generation\n",
        "- **Developer instructions** - System-level guidance separate from user content\n",
        "\n",
        "This notebook walks through the core features with practical examples, showing how you can integrate this API into your AI applications for cleaner, more maintainable code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWTBWGsdWeuw"
      },
      "source": [
        "### Setup and Authentication\n",
        "\n",
        "First, we need to set up our OpenAI API credentials. We'll use `getpass` to securely input the API key without exposing it in the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zCusHswP-lX",
        "outputId": "dbd8b121-72cc-406f-8d25-bac2dcd7914e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYqwOef1Weuw"
      },
      "source": [
        "Now we'll initialize the OpenAI client using our API key. This client will be used for all subsequent API calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAXziP4OQqVN"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcI8DQiNWeux"
      },
      "source": [
        "### Basic Response Generation\n",
        "\n",
        "Let's start with a simple example using the new `responses.create()` method. This is the core interface for the Responses API - notice how clean and straightforward it is compared to the traditional chat completions API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAEIt5nVQEFk",
        "outputId": "e2e03f31-3cf8-44d3-f445-cdc3f64c8be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI engineering is the discipline of systematically designing, building, deploying, and operating AI-enabled systems so they are reliable, safe, maintainable, and valuable in real-world use. It integrates machine learning with software, data, and systems engineering, along with governance and human-centered practices.\n",
            "\n",
            "Key elements include:\n",
            "- End-to-end lifecycle: problem framing, data and model development, evaluation, deployment, monitoring, and continuous improvement.\n",
            "- Architecture and infrastructure: data pipelines, feature stores, training/serving platforms, vector/RAG components, orchestration, and scalable infrastructure.\n",
            "- MLOps/LLMOps: versioning, CI/CD/CT for models and prompts, automated evaluations, rollout strategies (shadow, canary, A/B).\n",
            "- Quality, safety, and risk: robustness, reliability, security, privacy, fairness, interpretability, and rigorous testing/validation.\n",
            "- Operations: observability, drift and performance monitoring, incident response, cost/latency management.\n",
            "- Governance and compliance: documentation, auditability, policy controls, model cards, and responsible AI practices.\n",
            "- Human factors and product alignment: human-in-the-loop workflows, UX, feedback loops, and metrics tied to business outcomes.\n",
            "\n",
            "In short, AI engineering focuses on turning AI capabilities into trustworthy, scalable, and maintainable products and services.\n"
          ]
        }
      ],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    input=\"Define what 'AI Engineering' is.\"\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyqyQaEeWeux"
      },
      "source": [
        "### Reasoning Control and Instructions\n",
        "\n",
        "One of the powerful features of the Responses API is the ability to control the reasoning effort and provide developer instructions. Here we're using:\n",
        "\n",
        "- `reasoning={\"effort\": \"low\"}` - Controls how much computational effort the model puts into reasoning\n",
        "- `instructions` - Developer-level instructions that guide the model's behavior (separate from user content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdBe3I03Qbw3",
        "outputId": "55d77342-87e1-42fd-9c23-fb3c0d90a513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, seeker of speed, heed these arcane laws of NumPy, where loops of Pythonland are but lumbering ogres, and vectorized spells dance like lightning.\n",
            "\n",
            "Principles of swiftness\n",
            "- Prefer vectorization over Python loops: operate on whole arrays at once. NumPy’s ufuncs (sin, exp, add, etc.) run in C and are vastly faster.\n",
            "- Broadcast, don’t iterate: shape your arrays to align and let broadcasting do the work instead of nested loops.\n",
            "- Reduce along axes: use sum, mean, max, any, argmax, etc., with axis=... to avoid manual accumulation.\n",
            "- Preallocate: conjure the full output array once; do not grow lists or arrays inside loops.\n",
            "- Keep data contiguous and typed: use a consistent dtype and contiguous memory (arr.flags). Copy only when needed.\n",
            "- Use BLAS-backed ops: dot, matmul (@), einsum call into highly optimized libraries.\n",
            "- Avoid fake magic: np.vectorize and apply_along_axis look fancy but are still Python loops; they’re for convenience, not speed.\n",
            "- Minimize temporaries: fuse operations when you can; consider out=... to write results directly.\n",
            "\n",
            "Common incantations (with examples)\n",
            "- Elementwise transforms\n",
            "  - Instead of: for i: y[i] = a*x[i] + b if x[i]>t else c\n",
            "  - Do: y = np.where(x > t, a*x + b, c)\n",
            "\n",
            "- Reductions\n",
            "  - Total or per-axis: x.sum(), x.mean(axis=0), (x > 0).any(axis=1)\n",
            "  - Cumulative: np.cumsum(x), np.maximum.accumulate(x)\n",
            "\n",
            "- Broadcasting\n",
            "  - Pairwise distances (N×D vs M×D): \n",
            "    - diff = A[:, None, :] - B[None, :, :]\n",
            "    - d2 = np.einsum('ijk,ijk->ij', diff, diff)\n",
            "    - Or use scipy.spatial.distance.cdist if available.\n",
            "\n",
            "- Matrix-like spells\n",
            "  - Linear algebra: A @ B, A.T @ A, np.linalg.solve(A, b)\n",
            "  - General contractions: np.einsum('ij,j->i', A, x) instead of Python loops\n",
            "\n",
            "- Grouped/segmented magic\n",
            "  - For grouped sums with sorted keys:\n",
            "    - idx = np.flatnonzero(np.r_[True, keys[1:] != keys[:-1]])\n",
            "    - out = np.add.reduceat(values, idx)\n",
            "  - Or use pandas/Polars for complex group-bys.\n",
            "\n",
            "- Sliding windows and convolution\n",
            "  - Fast rolling ops: from numpy.lib.stride_tricks import sliding_window_view\n",
            "    - w = sliding_window_view(x, window_shape=K)\n",
            "    - sums = w.sum(axis=-1)\n",
            "  - 2D filters: use scipy.signal.convolve2d or ndimage for speed and clarity.\n",
            "\n",
            "- Sorting and selection\n",
            "  - Top-k: np.argpartition(x, -k)[-k:]\n",
            "  - Stable sort with key: np.lexsort((secondary, primary))\n",
            "\n",
            "- Boolean and index magic\n",
            "  - Masking: y = x[mask]\n",
            "  - Scatter/gather: out[idx] = vals, vals = x[idx]\n",
            "  - Choose/where: np.where(cond, a, b), np.take(along_axis, argmax, etc.)\n",
            "\n",
            "Performance charms and traps\n",
            "- Contiguity and dtype\n",
            "  - Ensure tight layout: arr = np.ascontiguousarray(arr) when needed.\n",
            "  - Avoid upcasting to float64 if float32 suffices; it halves bandwidth.\n",
            "- Avoid needless copies\n",
            "  - Views are free; slicing returns views. Prefer ravel(order='K') over flatten().\n",
            "  - astype(copy=False) where possible.\n",
            "- Chunk large work for memory\n",
            "  - For massive broadcasts, compute in tiles to control peak memory.\n",
            "- nditer rarely helps\n",
            "  - It’s still Python-level iteration; only useful for exotic stepping without allocation.\n",
            "- Parallelism comes from BLAS\n",
            "  - Large matmul/dot may already be multithreaded via MKL/OpenBLAS. Control threads via environment variables (e.g., OMP_NUM_THREADS).\n",
            "- Random realms\n",
            "  - Use Generator API (np.random.default_rng()). Vectorize draws: rng.normal(size=(n, m)).\n",
            "\n",
            "When loops are truly needed\n",
            "- Use Numba: @numba.njit on array-oriented code yields C-like speed while keeping NumPy syntax.\n",
            "- Use Cython or specialized libraries (numexpr for certain expressions).\n",
            "\n",
            "Mini before/after spellbook\n",
            "- Elementwise loop → vectorized ufuncs and where\n",
            "- Accumulating in Python → reductions with axis\n",
            "- Nested loops over pairs → broadcasting + einsum\n",
            "- Rolling window in Python → sliding_window_view or convolution\n",
            "- Building arrays in a loop → preallocate and assign; or accumulate lists, then np.array at the end\n",
            "\n",
            "If you bring me a concrete loop you wish to banish, I shall transmute it into swift vectorized sorcery tailored to your arrays.\n"
          ]
        }
      ],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    reasoning={\"effort\": \"low\"},\n",
        "    instructions=\"Talk like a wizard.\",\n",
        "    input=\"How to write an efficient loop with NumPy?\",\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycx8tykjWeux"
      },
      "source": [
        "### Message-Based Input Format\n",
        "\n",
        "The Responses API also supports the familiar message format with roles. Here we demonstrate:\n",
        "\n",
        "- Using `input` as a list of message objects instead of a simple string\n",
        "- `developer` role - A new role type for system-level instructions\n",
        "- `user` role - Standard user input\n",
        "\n",
        "This approach gives you more granular control over the conversation structure while maintaining the simplified API interface.\n",
        "\n",
        "> NOTE: This should be *roughly* equivalent to the above cell - since we're using the same \"instructions\" in our `developer` role.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1c3IhkyQnG7",
        "outputId": "ca083b8c-7eca-4f29-b518-52ea15cc815d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hark, seeker of swiftness! In the realm of NumPy, thou dost not loop as in mortal Python; thou summonest arrays to dance as one through vectorized sorcery. Behold these spells for efficient “loops”:\n",
            "\n",
            "- Prefer vectorization over Python loops\n",
            "  - Replace elementwise for-loops with array-wide operations (ufuncs): +, -, *, /, **, sqrt, sin, etc.\n",
            "  - Example: y = a*x + b is instant across the whole array x.\n",
            "\n",
            "- Wield broadcasting to align shapes without copies\n",
            "  - Let shapes expand with singleton dimensions instead of repeating data.\n",
            "  - Example (pairwise squared distances):\n",
            "    X: shape (n, d), Y: shape (m, d)\n",
            "    D2 = ((X[:, None, :] - Y[None, :, :])**2).sum(axis=-1)\n",
            "\n",
            "- Summon reductions along axes\n",
            "  - Use axis to avoid loops: sum, mean, max, argmax, std, any, all.\n",
            "  - Example: col_means = X.mean(axis=0)\n",
            "\n",
            "- Favor in-place enchantments to save memory and time\n",
            "  - a += b, a *= 2, or use out= and where= in ufuncs.\n",
            "  - Example: np.add(a, b, out=a)\n",
            "\n",
            "- Choose spells built on BLAS\n",
            "  - dot, matmul (@), einsum for multi-index contractions.\n",
            "  - Example: C = A @ B; s = np.einsum('ij,ij->', A, B)\n",
            "\n",
            "- Conjure boolean indexing and where instead of branching loops\n",
            "  - a[a < 0] = 0\n",
            "  - b = np.where(mask, x, y)\n",
            "\n",
            "- Preallocate arrays; shun repeated append\n",
            "  - res = np.empty(n); fill it, or compute in one vectorized shot.\n",
            "\n",
            "- Mind thy dtypes and memory layout\n",
            "  - Use float32/int32 if sufficient; ensure contiguous arrays (np.ascontiguousarray).\n",
            "  - Copying is costly; slicing with views is cheap.\n",
            "\n",
            "- Rolling and windowed rites\n",
            "  - Moving sum/average: use convolution or cumsum.\n",
            "  - mov_avg_k = np.convolve(x, np.ones(k)/k, mode='valid')\n",
            "  - Or via cumsum:\n",
            "    s = np.cumsum(np.r_[0, x]); mov = (s[k:] - s[:-k]) / k\n",
            "\n",
            "- Grouped operations\n",
            "  - If groups are small integers: np.bincount for sums/counts/means.\n",
            "    sums = np.bincount(groups, weights=values)\n",
            "    counts = np.bincount(groups)\n",
            "    means = sums / counts\n",
            "\n",
            "- Sorting, uniqueness, and set lore\n",
            "  - np.unique, np.argsort, np.partition for top-k without full sort.\n",
            "\n",
            "- Avoid these glamours (they look magic, but are slow)\n",
            "  - np.vectorize: convenience only; not faster than a Python loop.\n",
            "  - np.apply_along_axis: often slow; seek true vectorization instead.\n",
            "  - np.nditer: rarely needed for speed; stays in Python.\n",
            "\n",
            "- When vectorization becomes a hydra too large\n",
            "  - Very large intermediates? Chunk your work to limit memory.\n",
            "  - Or call just-in-time spirits: numba.njit on small tight loops over arrays.\n",
            "  - For elementwise arithmetic-heavy kernels, numexpr can help.\n",
            "\n",
            "- Profiling crystals\n",
            "  - Use %timeit in a notebook; for bigger hunts, perf_counter and asv.\n",
            "  - Ensure warm-up and avoid printing while timing.\n",
            "\n",
            "Tiny examples of replacing loops:\n",
            "\n",
            "- Normalize rows of a matrix\n",
            "  - X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
            "\n",
            "- Softmax per row (stable)\n",
            "  - Z = X - X.max(axis=1, keepdims=True)\n",
            "  - P = np.exp(Z); P /= P.sum(axis=1, keepdims=True)\n",
            "\n",
            "- Clip and scale with mask\n",
            "  - a = np.clip(a, lo, hi)\n",
            "  - a[mask] *= scale\n",
            "\n",
            "- Distance of each point to each center (large n,m beware memory)\n",
            "  - D2 = ((X[:, None, :] - C[None, :, :])**2).sum(-1)\n",
            "  - For memory safety: compute in chunks over m or n.\n",
            "\n",
            "If thou hast a specific loop to transmute, present it, and I shall recast it into swift NumPy incantations.\n"
          ]
        }
      ],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    reasoning={\"effort\": \"low\"},\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": \"Talk like a wizard.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How to write an efficient loop with NumPy?\"\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_g8cEf2Weux"
      },
      "source": [
        "### Structured Output with Pydantic\n",
        "\n",
        "One of the most exciting features is native structured output parsing using Pydantic models. Instead of trying to parse JSON from text responses, the API can directly return structured data.\n",
        "\n",
        "Key features:\n",
        "- `responses.parse()` method for structured output\n",
        "- `text_format` parameter accepts Pydantic models\n",
        "- Automatic validation and type checking\n",
        "- Clean, typed responses that integrate seamlessly with Python applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN_YzMK8Q7wd",
        "outputId": "3afea092-007d-4a7d-9199-8d6dbf0f8228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='AI Engineering Bootcamp kickoff' date='2025-09-09T19:00:00' participants=['Alice', 'Bob']\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class CalendarEvent(BaseModel):\n",
        "    name: str\n",
        "    date: str\n",
        "    participants: list[str]\n",
        "\n",
        "response = client.responses.parse(\n",
        "    model=\"gpt-5\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"developer\",\n",
        "            \"content\": \"Extract the event information.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Alice and Bob are going to the AI Engineering Bootcamp kickoff on September 9th at 7PM. Registration closes Tuesday (9/9) at Noon EDT.\",\n",
        "        },\n",
        "    ],\n",
        "    text_format=CalendarEvent,\n",
        ")\n",
        "\n",
        "print(response.output_parsed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUpvG3UbWeux"
      },
      "source": [
        "### Multimodal Input - Text and Images\n",
        "\n",
        "The Responses API provides excellent support for multimodal inputs, allowing you to combine text and images in a single request. This example shows:\n",
        "\n",
        "- `input_text` and `input_image` content types\n",
        "- Direct image URL support\n",
        "- Unified processing of text and visual information\n",
        "\n",
        "This makes it easy to build applications that need to analyze images, documents, or other visual content alongside text instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJqn5d6eRGpx",
        "outputId": "9471a0a6-2843-44b7-ed24-39ef1574273b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Person on the left:\n",
            "  - Adult man, light complexion, short facial hair/stubble.\n",
            "  - Wearing a black hoodie and a black cap with a white speckled pattern on the front.\n",
            "  - Smiling with teeth visible; facing slightly to the right.\n",
            "  - Outlined with a neon pink glow.\n",
            "\n",
            "- Person on the right:\n",
            "  - Adult man with hair pulled back and a short goatee/mustache.\n",
            "  - Wearing a blue-and-white cloud/tie-dye style polo shirt.\n",
            "  - Arms crossed and smiling.\n",
            "  - Wearing a dark smartwatch; outlined with a blue glow.\n"
          ]
        }
      ],
      "source": [
        "response = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"input_text\",\n",
        "                    \"text\": \"Describe the visual appearance of the people in this image.\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"input_image\",\n",
        "                    \"image_url\": \"https://d2426xcxuh3ht5.cloudfront.net/rnDUeTj9Sqysz9tCeGdB_AIE_Cohort_7_Banner.jpg\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbCclIQ7Weux"
      },
      "source": [
        "### Streaming Responses with Structured Output\n",
        "\n",
        "For real-time applications, the Responses API supports streaming even with structured output. This example demonstrates:\n",
        "\n",
        "- `responses.stream()` context manager for streaming\n",
        "- Event-based streaming with different event types:\n",
        "  - `response.output_text.delta` - Incremental text updates\n",
        "  - `response.refusal.delta` - Safety refusal messages  \n",
        "  - `response.error` - Error handling\n",
        "  - `response.completed` - Stream completion\n",
        "- `get_final_response()` to retrieve the complete parsed result\n",
        "- Structured output streaming with Pydantic models\n",
        "\n",
        "This is perfect for building responsive UIs that show progress while maintaining type safety.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD91RQ79Rupf",
        "outputId": "f5e9d98f-dcb5-4d43-9bcb-17a12d1ec767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"attributes\":[\"quick\",\"lazy\",\"piercing\"],\"colors\":[\"brown\",\"blue\"],\"animals\":[\"fox\",\"dog\"]}Completed\n",
            "ParsedResponse[EntitiesModel](id='resp_68b880fc55588192a367c429226609220bab634bdea19ee3', created_at=1756922108.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-5-mini-2025-08-07', object='response', output=[ResponseReasoningItem(id='rs_68b880fd9e148192afc973bb5dd588440bab634bdea19ee3', summary=[], type='reasoning', content=None, encrypted_content=None, status=None), ParsedResponseOutputMessage[EntitiesModel](id='msg_68b8810105348192a8e7641c0c1da5f90bab634bdea19ee3', content=[ParsedResponseOutputText[EntitiesModel](annotations=[], text='{\"attributes\":[\"quick\",\"lazy\",\"piercing\"],\"colors\":[\"brown\",\"blue\"],\"animals\":[\"fox\",\"dog\"]}', type='output_text', logprobs=[], parsed=EntitiesModel(attributes=['quick', 'lazy', 'piercing'], colors=['brown', 'blue'], animals=['fox', 'dog']))], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatTextJSONSchemaConfig(name='EntitiesModel', schema_={'properties': {'attributes': {'items': {'type': 'string'}, 'title': 'Attributes', 'type': 'array'}, 'colors': {'items': {'type': 'string'}, 'title': 'Colors', 'type': 'array'}, 'animals': {'items': {'type': 'string'}, 'title': 'Animals', 'type': 'array'}}, 'required': ['attributes', 'colors', 'animals'], 'title': 'EntitiesModel', 'type': 'object', 'additionalProperties': False}, type='json_schema', description=None, strict=True), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=103, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=419, output_tokens_details=OutputTokensDetails(reasoning_tokens=384), total_tokens=522), user=None, store=True)\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "class EntitiesModel(BaseModel):\n",
        "    attributes: List[str]\n",
        "    colors: List[str]\n",
        "    animals: List[str]\n",
        "\n",
        "with client.responses.stream(\n",
        "    model=\"gpt-5-mini\",\n",
        "    input=[\n",
        "        {\"role\": \"system\", \"content\": \"Extract entities from the input text\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"The quick brown fox jumps over the lazy dog with piercing blue eyes\",\n",
        "        },\n",
        "    ],\n",
        "    text_format=EntitiesModel,\n",
        ") as stream:\n",
        "    for event in stream:\n",
        "        if event.type == \"response.refusal.delta\":\n",
        "            print(event.delta, end=\"\")\n",
        "        elif event.type == \"response.output_text.delta\":\n",
        "            print(event.delta, end=\"\")\n",
        "        elif event.type == \"response.error\":\n",
        "            print(event.error, end=\"\")\n",
        "        elif event.type == \"response.completed\":\n",
        "            print(\"Completed\")\n",
        "            # print(event.response.output)\n",
        "\n",
        "    final_response = stream.get_final_response()\n",
        "    print(final_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rf-wLzEWeux"
      },
      "source": [
        "## Summary\n",
        "\n",
        "The OpenAI Responses API represents a significant step forward in making AI integration simpler and more powerful. Key takeaways:\n",
        "\n",
        "✅ **Cleaner code** - Less boilerplate, more focus on your application logic  \n",
        "✅ **Built-in structure** - Native Pydantic support eliminates JSON parsing headaches  \n",
        "✅ **Flexible control** - Fine-tune reasoning effort and output style  \n",
        "✅ **Multimodal ready** - Text and image inputs work seamlessly together  \n",
        "✅ **Production ready** - Streaming support for responsive applications  \n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Configuration Options\n",
        "\n",
        "The final example showcases some advanced configuration options:\n",
        "\n",
        "- `reasoning={\"effort\": \"minimal\"}` - Even lower computational effort for simple tasks\n",
        "- `text={\"verbosity\": \"low\"}` - Control output verbosity for concise responses\n",
        "- Fine-tuning the balance between speed, cost, and output quality\n",
        "\n",
        "This final configuration will give you the closest possible performance to GPT-4o, just in case you miss the old model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvl7dNQkSLgG",
        "outputId": "2f15dc71-5ffb-4aee-adf9-e7b2d97840d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silent loops hum low,  \n",
            "logic blossoms into light—  \n",
            "bugs blink, then fade out.\n"
          ]
        }
      ],
      "source": [
        "result = client.responses.create(\n",
        "    model=\"gpt-5\",\n",
        "    input=\"Write a haiku about code.\",\n",
        "    reasoning={ \"effort\": \"minimal\" },\n",
        "    text={ \"verbosity\": \"low\" },\n",
        ")\n",
        "\n",
        "print(result.output_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}